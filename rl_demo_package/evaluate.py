"""
è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½
"""
from stable_baselines3 import PPO
from env import DecisionEnv
import numpy as np

def evaluate_model(num_episodes=20, verbose=True):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    env = DecisionEnv()
    model = PPO.load("ppo_decision")
    
    # ç»Ÿè®¡æŒ‡æ ‡
    episode_rewards = []
    episode_lengths = []
    success_count = 0  # æˆåŠŸåˆ°è¾¾ç›®æ ‡
    collision_count = 0  # ç¢°æ’éšœç¢ç‰©
    timeout_count = 0  # è¶…æ—¶ï¼ˆè¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼‰
    
    action_names = {0: "up", 1: "down", 2: "left", 3: "right"}
    
    print("=" * 70)
    print("æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("=" * 70)
    print(f"è¯„ä¼° {num_episodes} ä¸ª episodes...\n")
    
    for episode in range(num_episodes):
        obs, _ = env.reset()
        total_reward = 0
        steps = 0
        episode_actions = []
        
        while steps < 200:  # å¢åŠ æœ€å¤§æ­¥æ•°
            action, _ = model.predict(obs, deterministic=True)
            action = int(action)  # ç¡®ä¿æ˜¯æ ‡é‡
            obs, reward, done, truncated, _ = env.step(action)
            
            total_reward += reward
            steps += 1
            episode_actions.append(action_names[action])
            
            if done or truncated:
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        
        # è®¡ç®—æœ€ç»ˆè·ç¦»
        dist_to_dest = np.linalg.norm(env.destination - env.ego_pos)
        dist_to_obs = np.linalg.norm(env.obs_pos - env.ego_pos)
        
        # åˆ¤æ–­episodeç»“æœ
        if dist_to_obs < 2.0:
            collision_count += 1
            result = "âŒ ç¢°æ’"
        elif dist_to_dest < 8.0:  # æ›´æ–°åˆ°è¾¾é˜ˆå€¼
            success_count += 1
            result = "âœ… æˆåŠŸ"
        else:
            timeout_count += 1
            result = "â±ï¸  è¶…æ—¶"
        
        if verbose:
            print(f"Episode {episode + 1:2d}: {result:8s} | "
                  f"å¥–åŠ±: {total_reward:6.2f} | "
                  f"æ­¥æ•°: {steps:3d} | "
                  f"åˆ°ç›®æ ‡: {dist_to_dest:5.2f}")
            if verbose and episode < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªepisodeçš„è¯¦ç»†åŠ¨ä½œåºåˆ—
                print(f"  åŠ¨ä½œåºåˆ—: {' -> '.join(episode_actions[:15])}")
                if len(episode_actions) > 15:
                    print(f"            ... (å…±{len(episode_actions)}æ­¥)")
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 70)
    print("æ€»ä½“ç»Ÿè®¡")
    print("=" * 70)
    print(f"æ€»Episodes: {num_episodes}")
    print(f"  âœ… æˆåŠŸåˆ°è¾¾: {success_count} ({success_count/num_episodes*100:.1f}%)")
    print(f"  âŒ ç¢°æ’:     {collision_count} ({collision_count/num_episodes*100:.1f}%)")
    print(f"  â±ï¸  è¶…æ—¶:     {timeout_count} ({timeout_count/num_episodes*100:.1f}%)")
    print()
    print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
    print(f"  æœ€å°: {np.min(episode_rewards):.2f}")
    print(f"  æœ€å¤§: {np.max(episode_rewards):.2f}")
    print()
    print(f"å¹³å‡Episodeé•¿åº¦: {np.mean(episode_lengths):.2f} Â± {np.std(episode_lengths):.2f} æ­¥")
    print(f"  æœ€çŸ­: {np.min(episode_lengths)} æ­¥")
    print(f"  æœ€é•¿: {np.max(episode_lengths)} æ­¥")
    print("=" * 70)
    
    # æ€§èƒ½è¯„ä¼°
    print("\næ€§èƒ½è¯„ä¼°:")
    success_rate = success_count / num_episodes
    if success_rate >= 0.8:
        print("  ğŸ‰ ä¼˜ç§€ï¼æ¨¡å‹è¡¨ç°å¾ˆå¥½")
    elif success_rate >= 0.5:
        print("  ğŸ‘ è‰¯å¥½ï¼æ¨¡å‹è¡¨ç°ä¸é”™")
    elif success_rate >= 0.3:
        print("  âš ï¸  ä¸€èˆ¬ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒ")
    else:
        print("  âŒ è¾ƒå·®ï¼Œå»ºè®®å¢åŠ è®­ç»ƒæ—¶é—´æˆ–è°ƒæ•´è¶…å‚æ•°")
    
    return {
        'success_rate': success_rate,
        'avg_reward': np.mean(episode_rewards),
        'avg_length': np.mean(episode_lengths),
        'success_count': success_count,
        'collision_count': collision_count,
        'timeout_count': timeout_count
    }

if __name__ == "__main__":
    # è¿è¡Œè¯„ä¼°
    results = evaluate_model(num_episodes=20, verbose=True)
    
    # å¯é€‰ï¼šè¿è¡Œä¸€æ¬¡è¯¦ç»†æ¼”ç¤º
    print("\n" + "=" * 70)
    print("è¯¦ç»†æ¼”ç¤º - å•ä¸ªEpisode")
    print("=" * 70)
    env = DecisionEnv()
    model = PPO.load("ppo_decision")
    
    obs, _ = env.reset(seed=42)
    action_names = {0: "up", 1: "down", 2: "left", 3: "right"}
    
    dist_to_dest = np.linalg.norm(env.destination - env.ego_pos)
    print(f"\nåˆå§‹çŠ¶æ€: ä½ç½®={env.ego_pos}, åˆ°ç›®æ ‡={dist_to_dest:.2f}\n")
    print(f"{'æ­¥éª¤':<6} {'åŠ¨ä½œ':<8} {'ä½ç½®':<20} {'åˆ°ç›®æ ‡':<10} {'å¥–åŠ±':<8} {'ç´¯è®¡å¥–åŠ±':<10}")
    print("-" * 80)
    
    total_reward = 0
    for step in range(200):
        action, _ = model.predict(obs, deterministic=True)
        action = int(action)  # ç¡®ä¿æ˜¯æ ‡é‡
        obs, reward, done, truncated, _ = env.step(action)
        total_reward += reward
        
        dist_to_dest = np.linalg.norm(env.destination - env.ego_pos)
        pos_str = f"({env.ego_pos[0]:.1f},{env.ego_pos[1]:.1f})"
        
        print(f"{step+1:<6} {action_names[action]:<8} {pos_str:<20} {dist_to_dest:>8.2f}  "
              f"{reward:>6.2f}  {total_reward:>8.2f}")
        
        if done or truncated:
            print(f"\nEpisodeç»“æŸï¼æœ€ç»ˆå¥–åŠ±: {total_reward:.2f}")
            dist_to_obs = np.linalg.norm(env.obs_pos - env.ego_pos)
            if dist_to_obs < 2.0:
                print("ç»“æœ: âŒ ç¢°æ’éšœç¢ç‰©")
            elif dist_to_dest < 8.0:  # æ›´æ–°åˆ°è¾¾é˜ˆå€¼
                print("ç»“æœ: âœ… æˆåŠŸåˆ°è¾¾ç›®æ ‡")
            else:
                print("ç»“æœ: â±ï¸  è¶…æ—¶")
            break
